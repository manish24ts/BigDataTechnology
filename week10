from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans, GaussianMixture, LDA
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.sql import Row
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("ClusteringExample").getOrCreate()

# Create a mock dataset
data = [
    Row(feature1=1.0, feature2=2.0), Row(feature1=1.5, feature2=1.8),
    Row(feature1=5.0, feature2=8.0), Row(feature1=8.0, feature2=8.0),
    Row(feature1=1.0, feature2=0.6), Row(feature1=9.0, feature2=11.0),
    Row(feature1=8.0, feature2=9.0), Row(feature1=7.0, feature2=6.0),
    Row(feature1=5.0, feature2=5.0), Row(feature1=6.0, feature2=4.0),
]
df = spark.createDataFrame(data)
print("Original Dataset:")
df.show()

# Assemble features
assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
assembled_df = assembler.transform(df)
evaluator = ClusteringEvaluator(predictionCol="prediction", featuresCol="features")

# Use all data for both training and testing in this demo
train_df = assembled_df
test_df = assembled_df

# K-Means
print("\n=== K-Means Clustering ===")
kmeans = KMeans(k=2, featuresCol="features", predictionCol="prediction", seed=123)
kmeans_model = kmeans.fit(train_df)
kmeans_predictions = kmeans_model.transform(test_df)
kmeans_silhouette = evaluator.evaluate(kmeans_predictions)
print(f"K-Means Silhouette Score: {kmeans_silhouette:.4f}")
kmeans_predictions.select("features", "prediction").show()

# GMM
print("\n=== Gaussian Mixture Model ===")
gmm = GaussianMixture(k=2, featuresCol="features", predictionCol="prediction", seed=123)
gmm_model = gmm.fit(train_df)
gmm_predictions = gmm_model.transform(test_df)
gmm_silhouette = evaluator.evaluate(gmm_predictions)
print(f"GMM Silhouette Score: {gmm_silhouette:.4f}")
gmm_predictions.select("features", "prediction").show()

# LDA - Using features as word counts (simplified for demo)
print("\n=== LDA Topic Modeling ===")
# Create non-negative integer features for LDA
lda_data = assembled_df.withColumn(
    "feature1_int", (col("feature1") * 10).cast("integer") + 1
).withColumn(
    "feature2_int", (col("feature2") * 10).cast("integer") + 1
)

# Re-assemble for LDA
lda_assembler = VectorAssembler(inputCols=["feature1_int", "feature2_int"], outputCol="word_counts")
lda_prepared = lda_assembler.transform(lda_data)

# Train LDA model
lda = LDA(k=2, featuresCol="word_counts", maxIter=10, seed=123)
lda_model = lda.fit(lda_prepared)
lda_results = lda_model.transform(lda_prepared)

# Show results
lda_model.describeTopics(2).show(truncate=False)
lda_results.select("word_counts", "topicDistribution").show(truncate=False)

spark.stop()
