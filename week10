from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans, GaussianMixture
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.topicmodeling import LDA
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.sql import Row

# Initialize Spark session
spark = SparkSession.builder.appName("ClusteringExample").getOrCreate()

# Create a mock dataset (example: two features for clustering)
data = [
    Row(feature1=1.0, feature2=2.0),
    Row(feature1=1.5, feature2=1.8),
    Row(feature1=5.0, feature2=8.0),
    Row(feature1=8.0, feature2=8.0),
    Row(feature1=1.0, feature2=0.6),
    Row(feature1=9.0, feature2=11.0),
    Row(feature1=8.0, feature2=9.0),
    Row(feature1=7.0, feature2=6.0),
    Row(feature1=5.0, feature2=5.0),
    Row(feature1=6.0, feature2=4.0),
]

# Create DataFrame from the mock dataset
df = spark.createDataFrame(data)

# Show the original dataset
df.show()

# Step 1: Assemble features into a single vector column
assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
assembled_df = assembler.transform(df)

# Step 2: Split the data into training and testing sets (80% training, 20% testing)
train_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=123)

# Step 3: Initialize K-Means model
kmeans = KMeans(k=2, featuresCol="features", predictionCol="prediction")

# Step 4: Train K-Means model
kmeans_model = kmeans.fit(train_df)

# Step 5: Make predictions using K-Means model
kmeans_predictions = kmeans_model.transform(test_df)

# Step 6: Evaluate K-Means model
evaluator = ClusteringEvaluator(predictionCol="prediction")
kmeans_silhouette = evaluator.evaluate(kmeans_predictions)
print(f"K-Means Silhouette Score: {kmeans_silhouette:.4f}")

# Step 7: Initialize Gaussian Mixture Model (GMM)
gmm = GaussianMixture(k=2, featuresCol="features", predictionCol="prediction")

# Step 8: Train GMM model
gmm_model = gmm.fit(train_df)

# Step 9: Make predictions using GMM model
gmm_predictions = gmm_model.transform(test_df)

# Step 10: Evaluate GMM model
gmm_silhouette = evaluator.evaluate(gmm_predictions)
print(f"GMM Silhouette Score: {gmm_silhouette:.4f}")

# Step 11: Initialize LDA for topic modeling (LDA requires a `document` format, so we'll simulate this for the example)
# For topic modeling, we'll assume we have a bag-of-words (BoW) representation of text data.
# Here we use features from the original dataset as "documents", for simplicity.

lda = LDA(k=2, featuresCol="features", predictionCol="topic")

# Step 12: Train LDA model
lda_model = lda.fit(train_df)

# Step 13: Make predictions using LDA model
lda_predictions = lda_model.transform(test_df)

# Step 14: Show LDA topics and predictions
print("LDA Topics and Probabilities:")
lda_model.describeTopics(5).show()

# Show the LDA predictions
lda_predictions.select("features", "topic").show()

# Show the predictions from K-Means and GMM models
print("K-Means Predictions:")
kmeans_predictions.select("features", "prediction").show()

print("GMM Predictions:")
gmm_predictions.select("features", "prediction").show()

# Stop the Spark session
spark.stop()
